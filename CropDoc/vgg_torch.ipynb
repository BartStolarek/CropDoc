{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vgg_class\n",
    "from data import DatasetManager, TransformerManager\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, CPUOffload\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize((224,224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize((224,224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/train_Data/Cashew/\"\n",
    "#test_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/test_data/Cashew/\"\n",
    "root_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/\"\n",
    "full_set = DatasetManager(root_dir, transform=data_standard_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30  # Number of passes through entire training dataset\n",
    "CV_FOLDS = 2  # Number of cross-validation folds\n",
    "BATCH_SIZE = 128  # Within each epoch data is split into batches\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = 0.1\n",
    "CROSS_VALIDATE = True\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "vgg = vgg_class.vgg16((len(full_set.unique_crops), len(full_set.unique_states)))\n",
    "model = nn.DataParallel(vgg, device_ids=device_ids)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion_crop = nn.CrossEntropyLoss()\n",
    "criterion_state = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30:   0%|          | 0/30 [02:12<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30: 63it [14:51, 14.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Train - Crop Loss: 0.927922600033009, State Loss: 2.286305971061234, Crop Accuracy: 60.41055312551907, State Accuracy: 28.546315264935497\n",
      "Validate - Crop Loss: 1.083845615386963, State Loss: 2.321791172027588, Crop Accuracy: 63.23657655412981, State Accuracy: 31.593372368257132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 63it [14:24, 13.72s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2\n",
      "Train - Crop Loss: 0.8170614701456729, State Loss: 2.126362614294069, Crop Accuracy: 67.02840374287138, State Accuracy: 32.45944299872654\n",
      "Validate - Crop Loss: 0.7233901619911194, State Loss: 1.9474072456359863, Crop Accuracy: 69.81437647938209, State Accuracy: 36.726049582658526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 63it [14:24, 13.72s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3\n",
      "Train - Crop Loss: 0.7134476766122126, State Loss: 1.9593547185965343, Crop Accuracy: 72.03504789325065, State Accuracy: 36.66186811361497\n",
      "Validate - Crop Loss: 0.7408238053321838, State Loss: 1.8223164081573486, Crop Accuracy: 74.47365142643578, State Accuracy: 41.79643702504049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 63it [14:24, 13.72s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4\n",
      "Train - Crop Loss: 0.5959273016558284, State Loss: 1.7417457894941346, Crop Accuracy: 77.64935496373401, State Accuracy: 43.01533691379215\n",
      "Validate - Crop Loss: 0.729225754737854, State Loss: 1.763702392578125, Crop Accuracy: 73.10327644200822, State Accuracy: 44.823719945185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 63it [14:26, 13.76s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "Train - Crop Loss: 0.4578528957820572, State Loss: 1.478653730122389, Crop Accuracy: 83.41315541775096, State Accuracy: 50.71286196777587\n",
      "Validate - Crop Loss: 0.5135307312011719, State Loss: 1.4237726926803589, Crop Accuracy: 86.47066151737884, State Accuracy: 56.04833686308708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 63it [14:22, 13.69s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6\n",
      "Train - Crop Loss: 0.3483661773985466, State Loss: 1.27450183872628, Crop Accuracy: 87.72770057028957, State Accuracy: 56.88915342450584\n",
      "Validate - Crop Loss: 0.2884375751018524, State Loss: 1.277924656867981, Crop Accuracy: 88.33935467796188, State Accuracy: 61.74162202566339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 63it [14:24, 13.71s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7\n",
      "Train - Crop Loss: 0.26668982690414494, State Loss: 1.1274892781688048, Crop Accuracy: 90.74248380488345, State Accuracy: 61.102652123359725\n",
      "Validate - Crop Loss: 0.3660416305065155, State Loss: 0.9272240996360779, Crop Accuracy: 91.15485237324032, State Accuracy: 63.69752086707363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 63it [14:21, 13.67s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8\n",
      "Train - Crop Loss: 0.21340127605780035, State Loss: 1.0092492262874029, Crop Accuracy: 92.6789768008416, State Accuracy: 64.61297824040751\n",
      "Validate - Crop Loss: 0.16356699168682098, State Loss: 0.9788539409637451, Crop Accuracy: 92.5875171296873, State Accuracy: 66.96150492089198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 63it [14:24, 13.73s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9\n",
      "Train - Crop Loss: 0.17708331791568646, State Loss: 0.9183808947031477, Crop Accuracy: 94.05348541055312, State Accuracy: 67.49072587342893\n",
      "Validate - Crop Loss: 0.10138192772865295, State Loss: 0.7523965239524841, Crop Accuracy: 89.37336489348449, State Accuracy: 66.0894481126199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 63it [14:21, 13.67s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10\n",
      "Train - Crop Loss: 0.1485565971220489, State Loss: 0.8407211533689921, Crop Accuracy: 94.86877803000941, State Accuracy: 70.270195448757\n",
      "Validate - Crop Loss: 0.23962868750095367, State Loss: 0.9683148860931396, Crop Accuracy: 95.14139778248412, State Accuracy: 72.76691167310328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 63it [14:20, 13.66s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11\n",
      "Train - Crop Loss: 0.1311324439810968, State Loss: 0.7810638992132338, Crop Accuracy: 95.62593433364708, State Accuracy: 72.56934831958364\n",
      "Validate - Crop Loss: 0.28549662232398987, State Loss: 1.1863150596618652, Crop Accuracy: 95.42793073377351, State Accuracy: 73.61405257256759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 63it [14:22, 13.69s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12\n",
      "Train - Crop Loss: 0.10576773248283208, State Loss: 0.7207192855598652, Crop Accuracy: 96.47306350700404, State Accuracy: 74.17917058856099\n",
      "Validate - Crop Loss: 0.18655888736248016, State Loss: 1.0162231922149658, Crop Accuracy: 94.35654665503924, State Accuracy: 73.90058552385699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 63it [14:18, 13.63s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13\n",
      "Train - Crop Loss: 0.0958072442602597, State Loss: 0.669869138076242, Crop Accuracy: 96.81911300592436, State Accuracy: 76.2665411660484\n",
      "Validate - Crop Loss: 0.06745331734418869, State Loss: 0.48356327414512634, Crop Accuracy: 92.3134421328018, State Accuracy: 75.19621278186122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 63it [14:16, 13.59s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14\n",
      "Train - Crop Loss: 0.08224779903987604, State Loss: 0.6119066838142091, Crop Accuracy: 97.26344056253807, State Accuracy: 78.11998228226565\n",
      "Validate - Crop Loss: 0.16302570700645447, State Loss: 0.8290718793869019, Crop Accuracy: 97.65790457206927, State Accuracy: 79.74336613927993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 63it [14:20, 13.66s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15\n",
      "Train - Crop Loss: 0.06957509984155144, State Loss: 0.5716160736252777, Crop Accuracy: 97.69946293117768, State Accuracy: 79.6273738995626\n",
      "Validate - Crop Loss: 0.16126424074172974, State Loss: 0.481723815202713, Crop Accuracy: 97.12221253270214, State Accuracy: 80.26660022424318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 63it [14:29, 13.80s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16\n",
      "Train - Crop Loss: 0.0640770833733269, State Loss: 0.5431169963515965, Crop Accuracy: 97.8974032445601, State Accuracy: 80.62399645645313\n",
      "Validate - Crop Loss: 0.013813738711178303, State Loss: 0.7008064985275269, Crop Accuracy: 96.22523981562227, State Accuracy: 81.27569453095802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 63it [14:24, 13.72s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17\n",
      "Train - Crop Loss: 0.05280443345798196, State Loss: 0.49150826261106845, Crop Accuracy: 98.23930014949339, State Accuracy: 82.37915951497702\n",
      "Validate - Crop Loss: 0.02114492654800415, State Loss: 0.47163376212120056, Crop Accuracy: 98.3929238818986, State Accuracy: 85.4740251650679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 63it [14:23, 13.70s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  18\n",
      "Train - Crop Loss: 0.04637019256690303, State Loss: 0.455460947541009, Crop Accuracy: 98.53275012457782, State Accuracy: 83.58894856320248\n",
      "Validate - Crop Loss: 0.04937303811311722, State Loss: 0.4832668900489807, Crop Accuracy: 98.1063909306092, State Accuracy: 85.6484365267223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 63it [14:21, 13.68s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  19\n",
      "Train - Crop Loss: 0.041342056147441006, State Loss: 0.4225125308585378, Crop Accuracy: 98.65732794418913, State Accuracy: 84.70322794972593\n",
      "Validate - Crop Loss: 0.01961478590965271, State Loss: 0.3747110664844513, Crop Accuracy: 94.95452846642581, State Accuracy: 83.50566836925377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 63it [14:22, 13.69s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  20\n",
      "Train - Crop Loss: 0.05992392793800517, State Loss: 0.4530438459552495, Crop Accuracy: 98.04689662809368, State Accuracy: 83.92669287414871\n",
      "Validate - Crop Loss: 0.07284712791442871, State Loss: 0.22173482179641724, Crop Accuracy: 99.37710227980565, State Accuracy: 90.10838420331382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 63it [14:25, 13.74s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  21\n",
      "Train - Crop Loss: 0.035656505408985295, State Loss: 0.36272027801095913, Crop Accuracy: 98.85942085155861, State Accuracy: 86.89441337688943\n",
      "Validate - Crop Loss: 0.03556832671165466, State Loss: 0.3061521053314209, Crop Accuracy: 99.40201818861343, State Accuracy: 89.85922511523607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 63it [14:20, 13.67s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  22\n",
      "Train - Crop Loss: 0.028727186435062967, State Loss: 0.33033929720389105, Crop Accuracy: 99.10857649078125, State Accuracy: 88.2218592547478\n",
      "Validate - Crop Loss: 0.14149852097034454, State Loss: 0.38790586590766907, Crop Accuracy: 99.56397159586396, State Accuracy: 91.19222623645197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 63it [14:24, 13.73s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  23\n",
      "Train - Crop Loss: 0.057048513115367204, State Loss: 0.39336901133039354, Crop Accuracy: 98.171474447705, State Accuracy: 86.35319196057804\n",
      "Validate - Crop Loss: 0.007656214293092489, State Loss: 0.3346252739429474, Crop Accuracy: 99.19023296374735, State Accuracy: 92.25115236078236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 63it [14:22, 13.69s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  24\n",
      "Train - Crop Loss: 0.02557766399443133, State Loss: 0.2888088541616381, Crop Accuracy: 99.18193898455235, State Accuracy: 89.65311998228226\n",
      "Validate - Crop Loss: 0.008745172992348671, State Loss: 0.5424212217330933, Crop Accuracy: 99.26498069017067, State Accuracy: 93.37236825713218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 63it [14:26, 13.75s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  25\n",
      "Train - Crop Loss: 0.0240809016005678, State Loss: 0.26321737813738594, Crop Accuracy: 99.24007530037096, State Accuracy: 90.69957366701733\n",
      "Validate - Crop Loss: 0.05922875180840492, State Loss: 0.5781629085540771, Crop Accuracy: 99.5141397782484, State Accuracy: 93.83331257007599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 63it [14:25, 13.74s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26\n",
      "Train - Crop Loss: 0.02941962128275371, State Loss: 0.2665592099317407, Crop Accuracy: 99.08227672886329, State Accuracy: 90.78816233874093\n",
      "Validate - Crop Loss: 0.10333777964115143, State Loss: 0.46111059188842773, Crop Accuracy: 99.2400647813629, State Accuracy: 94.3067148374237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 63it [14:24, 13.72s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  27\n",
      "Train - Crop Loss: 0.02238238673653058, State Loss: 0.22811759199980086, Crop Accuracy: 99.27606444825868, State Accuracy: 91.99103039698798\n",
      "Validate - Crop Loss: 0.02159169688820839, State Loss: 0.41418853402137756, Crop Accuracy: 99.73838295751838, State Accuracy: 95.41547277936962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 63it [14:18, 13.62s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  28\n",
      "Train - Crop Loss: 0.020527247825585432, State Loss: 0.20713606360739312, Crop Accuracy: 99.33143236808593, State Accuracy: 92.84369636232766\n",
      "Validate - Crop Loss: 0.007107607088983059, State Loss: 0.10762706398963928, Crop Accuracy: 99.66363523109506, State Accuracy: 95.09156596486856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 63it [14:26, 13.75s/it]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  29\n",
      "Train - Crop Loss: 0.018889584143484316, State Loss: 0.20153098953640566, Crop Accuracy: 99.40756325784841, State Accuracy: 92.96965837993467\n",
      "Validate - Crop Loss: 0.00191740901209414, State Loss: 0.048616524785757065, Crop Accuracy: 98.3929238818986, State Accuracy: 93.58415348199826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 63it [14:18, 13.63s/it]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  30\n",
      "Train - Crop Loss: 0.019456920039297113, State Loss: 0.19268680536641483, Crop Accuracy: 99.38403189192182, State Accuracy: 93.47489064835834\n",
      "Validate - Crop Loss: 0.007226305082440376, State Loss: 0.05732180178165436, Crop Accuracy: 99.88787841036502, State Accuracy: 96.98517503425937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define cross-validation iterator\n",
    "#skf = StratifiedShuffleSplit(n_splits=CV_FOLDS, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "# Determine the number of splits\n",
    "#n_splits = skf.get_n_splits(full_set.samples, full_set.targets)\n",
    "\n",
    "train_loss_crop = []\n",
    "train_loss_state = []\n",
    "val_loss_crop = []\n",
    "val_loss_state = []\n",
    "train_accuracy_crop = []\n",
    "train_accuracy_state = []\n",
    "val_accuracy_crop = []\n",
    "val_accuracy_state = []\n",
    "train_total = 0\n",
    "epoch_stats = []\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_crop_loss = 0.0\n",
    "    running_state_loss = 0.0\n",
    "    val_running_crop_loss = 0.0\n",
    "    val_running_state_loss = 0.0\n",
    "    crop_correct = 0\n",
    "    val_crop_correct = 0\n",
    "    val_state_correct = 0\n",
    "    state_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize tqdm for epoch progress\n",
    "    epoch_progress = tqdm(total=NUM_EPOCHS, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    '''\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_set.samples, train_set.targets)):\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "            \n",
    "    \n",
    "        val_loader = torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE, sampler=val_sampler, pin_memory=True)\n",
    "    '''\n",
    "    train, valid = random_split(full_set.train_samples, [1-VAL_SPLIT, VAL_SPLIT])\n",
    "    train_loader = DataLoader(train, batch_size=BATCH_SIZE ,pin_memory= True)\n",
    "    valid_loader = DataLoader(valid, batch_size=BATCH_SIZE ,pin_memory= True)\n",
    "            # Initialize tqdm for fold progress\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        img_paths = batch['img_path']\n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        images = []\n",
    "        for path, split in zip(img_paths, splits):\n",
    "            images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "            #fold_progress = tqdm(total=len(train_loader), desc=f'Fold {batch_idx + 1}/{len(train_loader)}', leave=False)\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "            #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "        inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "        crop_labels = crop_label_idx.clone().detach()\n",
    "        state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        crop_labels = crop_labels.to(device)\n",
    "        state_labels = state_labels.to(device)\n",
    "                # Zero the parameter gradients\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Forward pass\n",
    "        crop_outputs, state_outputs = model(inputs)\n",
    "            \n",
    "            #crop_outputs = model_outputs[:, :len(full_set.unique_crops)]\n",
    "            #state_outputs = model_outputs[:, len(full_set.unique_states):]\n",
    "\n",
    "                # Calculate loss\n",
    "        crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        state_loss = criterion_state(state_outputs, state_labels)\n",
    "            \n",
    "        total_loss = crop_loss + state_loss\n",
    "            #running_loss = running_loss + crop_loss + state_loss    \n",
    "                # Backward pass and optimize\n",
    "            #crop_loss.backward(retain_graph=True)\n",
    "            #state_loss.backward()\n",
    "            \n",
    "\n",
    "        _, predicted_crop = torch.max(crop_outputs, 1)\n",
    "        _, predicted_state = torch.max(state_outputs, 1)\n",
    "                        \n",
    "        crop_correct += (predicted_crop == crop_labels).sum().item()\n",
    "        state_correct += (predicted_state == state_labels).sum().item()\n",
    "        total += len(batch)\n",
    "\n",
    "        running_crop_loss+= crop_loss.item()\n",
    "        running_state_loss += state_loss.item()\n",
    "\n",
    "        crop_loss.backward(retain_graph=True)\n",
    "        state_loss.backward()\n",
    "        #total_loss.backward()\n",
    "            \n",
    "        optimiser.step()\n",
    "                # Update tqdm progress description at batch level\n",
    "            #fold_progress.set_postfix(loss=running_loss / (batch_idx + 1), accuracy=100. * correct / total)\n",
    "            #fold_progress.set_postfix({})\n",
    "            #fold_progress.update(1)\n",
    "\n",
    "            # Close fold progress bar\n",
    "            #fold_progress.close()\n",
    "\n",
    "            # Update tqdm progress at fold level\n",
    "            #if batch_idx == 1:\n",
    "            #    break\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(valid_loader):\n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        img_paths = batch['img_path']\n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        images = []\n",
    "        for path, split in zip(img_paths, splits):\n",
    "            images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "            #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "        inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "        crop_labels = crop_label_idx.clone().detach()\n",
    "        state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        crop_labels = crop_labels.to(device)\n",
    "        state_labels = state_labels.to(device)\n",
    "\n",
    "        crop_outputs, state_outputs = model(inputs)\n",
    "\n",
    "        val_crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        val_state_loss = criterion_state(state_outputs, state_labels)\n",
    "\n",
    "        _, val_predicted_crop = torch.max(crop_outputs, 1)\n",
    "        _, val_predicted_state = torch.max(state_outputs, 1)\n",
    "\n",
    "        val_crop_correct += (val_predicted_crop == crop_labels).sum().item()\n",
    "        val_state_correct += (val_predicted_state == state_labels).sum().item()\n",
    "        total += len(batch)\n",
    "\n",
    "        val_running_crop_loss+= crop_loss.item()\n",
    "        val_running_state_loss += state_loss.item()\n",
    "\n",
    "        epoch_progress.update(1)\n",
    "\n",
    "    # Close epoch progress bar\n",
    "    epoch_progress.close()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_crop_loss = running_crop_loss / len(train_loader)\n",
    "    epoch_state_loss = running_state_loss / len(train_loader)\n",
    "    epoch_crop_accuracy = 100. * crop_correct / len(train)\n",
    "    epoch_state_accuracy = 100. * state_correct / len(train)\n",
    "    epoch_val_crop_loss = val_running_crop_loss / len(valid_loader)\n",
    "    epoch_val_state_loss = val_running_state_loss / len(valid_loader)\n",
    "    epoch_val_crop_accuracy = 100. * val_crop_correct / len(valid)\n",
    "    epoch_val_state_accuracy = 100. * val_state_correct / len(valid)\n",
    "\n",
    "    # Log metrics\n",
    "    train_loss_crop.append(epoch_crop_loss)\n",
    "    train_loss_state.append(epoch_state_loss)\n",
    "    train_accuracy_crop.append(epoch_crop_accuracy)\n",
    "    train_accuracy_state.append(epoch_state_accuracy)\n",
    "    val_loss_crop.append(epoch_val_crop_loss)\n",
    "    val_loss_state.append(epoch_val_state_loss)\n",
    "    val_accuracy_crop.append(epoch_val_crop_accuracy)\n",
    "    val_accuracy_state.append(epoch_val_state_accuracy)\n",
    "    print(\"Epoch: \", epoch + 1)\n",
    "    print(\"Train - Crop Loss: {}, State Loss: {}, Crop Accuracy: {}, State Accuracy: {}\".format(train_loss_crop[epoch], train_loss_state[epoch], train_accuracy_crop[epoch], train_accuracy_state[epoch]))\n",
    "    print(\"Validate - Crop Loss: {}, State Loss: {}, Crop Accuracy: {}, State Accuracy: {}\".format(val_loss_crop[epoch], val_loss_state[epoch], val_accuracy_crop[epoch], val_accuracy_state[epoch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop correct:  78774\n",
      "State correct:  27017\n",
      "Total:  80271\n",
      "crop_accuracy:  98.13506745898269\n"
     ]
    }
   ],
   "source": [
    "print(\"Crop correct: \", crop_correct)\n",
    "print(\"State correct: \", state_correct)\n",
    "print(\"Total: \", len(full_set.train_samples))\n",
    "print(\"crop_accuracy: \", 100. * crop_correct / len(full_set.train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y+6\n"
     ]
    }
   ],
   "source": [
    "y = 2\n",
    "print(\"y+6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cashew': 18910, 'cassava': 20212, 'maize': 19426, 'tomato': 21723}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Crop Loss:  1.2459352704276025\n",
      "State Loss:  2.749915881283515\n",
      "Crop Accuracy:  50.53983721831571\n",
      "State Accuracy:  20.99136260450695\n",
      "Validation Crop Loss:  1.061446189880371\n",
      "Validation States Loss:  2.699982166290283\n",
      "Validation Crop Accuracy:  58.37797433661393\n",
      "Validation States Accuracy:  28.80279058178647\n",
      "Epoch:  2\n",
      "Crop Loss:  0.9305367541524161\n",
      "State Loss:  2.2799721538493065\n",
      "Crop Accuracy:  60.845191296163\n",
      "State Accuracy:  28.95465367366148\n",
      "Validation Crop Loss:  0.9835046529769897\n",
      "Validation States Loss:  2.45900559425354\n",
      "Validation Crop Accuracy:  64.74398903700012\n",
      "Validation States Accuracy:  28.877538308209793\n",
      "Epoch:  3\n",
      "Crop Loss:  0.8069555794243264\n",
      "State Loss:  2.0620343174554607\n",
      "Crop Accuracy:  67.39244781573557\n",
      "State Accuracy:  34.64509163390731\n",
      "Validation Crop Loss:  0.7115528583526611\n",
      "Validation States Loss:  1.7276532649993896\n",
      "Validation Crop Accuracy:  57.61803911797683\n",
      "Validation States Accuracy:  31.207175781736638\n",
      "Epoch:  4\n",
      "Crop Loss:  0.675626997610109\n",
      "State Loss:  1.8064118423293123\n",
      "Crop Accuracy:  74.15010243065169\n",
      "State Accuracy:  41.746027351752396\n",
      "Validation Crop Loss:  0.44347113370895386\n",
      "Validation States Loss:  1.4403612613677979\n",
      "Validation Crop Accuracy:  80.29151613305095\n",
      "Validation States Accuracy:  49.184003986545406\n",
      "Epoch:  5\n",
      "Crop Loss:  0.54235997600893\n",
      "State Loss:  1.5510228176032548\n",
      "Crop Accuracy:  79.89729250872045\n",
      "State Accuracy:  48.5784286584353\n",
      "Validation Crop Loss:  0.465952068567276\n",
      "Validation States Loss:  1.5935490131378174\n",
      "Validation Crop Accuracy:  70.42481624517254\n",
      "Validation States Accuracy:  42.36950292761929\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    print(\"Epoch: \", i+1)\n",
    "    print(\"Crop Loss: \", train_loss_crop[i])\n",
    "    print(\"State Loss: \", train_loss_state[i])\n",
    "    print(\"Crop Accuracy: \", train_accuracy_crop[i])\n",
    "    print(\"State Accuracy: \", train_accuracy_state[i])\n",
    "    print(\"Validation Crop Loss: \", val_loss_crop[i])\n",
    "    print(\"Validation States Loss: \", val_loss_state[i])\n",
    "    print(\"Validation Crop Accuracy: \", val_accuracy_crop[i])\n",
    "    print(\"Validation States Accuracy: \", val_accuracy_state[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "crop_correct = 0\n",
    "state_correct = 0\n",
    "model.eval()\n",
    "crop_output_list = []\n",
    "state_output_list = []\n",
    "         #torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE)\n",
    "testing = torch.utils.data.DataLoader(full_set.test_samples)\n",
    "\n",
    "for batch_idx, batch in enumerate(testing):\n",
    "    crop_label_idx = batch['crop_idx']\n",
    "    img_paths = batch['img_path']\n",
    "    splits = batch['split']\n",
    "    state_label_idx = batch['state_idx']\n",
    "    images = []\n",
    "    for path, split in zip(img_paths, splits):\n",
    "        images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "    images_tensor = torch.stack(images, dim=0)\n",
    "    #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "    inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "    crop_labels = crop_label_idx.clone().detach()\n",
    "    state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    crop_labels = crop_labels.to(device)\n",
    "    state_labels = state_labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "    crop_outputs, state_outputs = model(inputs)\n",
    "\n",
    "    _, crop_predicted = crop_outputs.max(1)\n",
    "    _, state_predicted = state_outputs.max(1)\n",
    "    total += crop_labels.size(0)\n",
    "    crop_correct += crop_predicted.eq(crop_labels).sum().item()\n",
    "    state_correct += state_predicted.eq(state_labels).sum().item()\n",
    "\n",
    "    crop_output_list.append({\"Path\":path,\n",
    "                             \"Label\":crop_labels,\n",
    "                             \"Predicted\":crop_predicted,\n",
    "                             \"Output\":torch.nn.functional.softmax(crop_outputs)})\n",
    "    \n",
    "    state_output_list.append({\"Path\":path,\n",
    "                             \"Label\":state_labels,\n",
    "                             \"Predicted\":state_predicted,\n",
    "                             \"Output\":torch.nn.functional.softmax(state_outputs)})\n",
    "    \n",
    "#with torch.no_grad():\n",
    "#    pred = model(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Test Accuracy:  91.11324606701093\n",
      "Crop Test Accuracy:  98.59893519074497\n"
     ]
    }
   ],
   "source": [
    "crop_accuracy = 100. * crop_correct / len(full_set.test_samples)\n",
    "state_accuracy = 100. * state_correct / len(full_set.test_samples)\n",
    "print(\"State Test Accuracy: \", state_accuracy)\n",
    "print(\"Crop Test Accuracy: \", crop_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24981"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.nn.functional.softmax(crop_outputs)\n",
    "len(full_set.test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/braines/Dataset/CCMT-Dataset-Augmented/test_data/Maize/leaf beetle/714maize_valid_leaf beetle.JPG']\n",
      "tensor([3])\n",
      "tensor([3])\n",
      "['tomato', 'cassava', 'cashew', 'maize']\n",
      "tensor([4])\n",
      "['leaf spot', 'streak virus', 'leaf curl', 'mosaic', 'leaf beetle', 'verticulium wilt', 'anthracnose', 'grasshoper', 'bspot', 'cashew healthy', 'bb', 'cassava healthy', 'septoria leaf spot', 'tomato healthy', 'gumosis', 'leaf miner', 'farmyw', 'maize healthy', 'leaf blight', 'gmite', 'red rust']\n"
     ]
    }
   ],
   "source": [
    "crop_label_test = []\n",
    "state_label_test = []\n",
    "testing = torch.utils.data.DataLoader(full_set.test_samples)\n",
    "count = 0\n",
    "for batch_idx, batch in enumerate(testing):\n",
    "        \n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        if crop_label_idx not in crop_label_test:\n",
    "                crop_label_test.append(crop_label_idx)\n",
    "        img_paths = batch['img_path']\n",
    "        \n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        if state_label_idx not in state_label_test:\n",
    "                state_label_test.append(state_label_idx)\n",
    "        \n",
    "        if batch_idx == 16000:\n",
    "                print(img_paths)\n",
    "                print(crop_label_idx)\n",
    "                print(crop_label_idx.clone().detach())\n",
    "                print(full_set.unique_crops)\n",
    "                print(state_label_idx)\n",
    "                print(full_set.unique_states)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crop_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/scratch/braines/vgg16/best_model_30_epoch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: Union[str, Path]) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folders in a dataset.\n",
    "\n",
    "    See :class:`DatasetFolder` for details.\n",
    "    \"\"\"\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_classes(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_mean_and_sd(loader):\n",
    "    \"\"\"Compute the mean and sd in an online fashion\n",
    "\n",
    "        Var[x] = E[X^2] - E^2[X]\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for data in loader:\n",
    "\n",
    "        b, c, h, w = data.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(data, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(data ** 2, dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_set = torch.utils.data.DataLoader(full_set.test_sample, batch_size=1, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in normal_set:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(normal_set.dataset)\n",
    "std /= len(normal_set.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.4851, 0.5189, 0.3830])\n",
      "Std:  tensor([0.2000, 0.1880, 0.2216])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CropDoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
