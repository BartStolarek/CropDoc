{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vgg_class\n",
    "from data import DatasetManager, TransformerManager\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, CPUOffload\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard_transforms = {\n",
    "    \"train\": transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "    \"test\": transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/train_Data/Cashew/\"\n",
    "#test_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/test_data/Cashew/\"\n",
    "root_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/\"\n",
    "full_set = DatasetManager(root_dir, transform=data_standard_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1  # Number of passes through entire training dataset\n",
    "CV_FOLDS = 2  # Number of cross-validation folds\n",
    "BATCH_SIZE = 64  # Within each epoch data is split into batches\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = 0.2\n",
    "CROSS_VALIDATE = True\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "vgg = vgg_class.vgg16((len(full_set.unique_crops), len(full_set.unique_states)))\n",
    "model = nn.DataParallel(vgg, device_ids=device_ids)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion_crop = nn.CrossEntropyLoss()\n",
    "criterion_state = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/braines/.conda/envs/vgg/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "Epoch 1/5: 2509it [49:52,  1.19s/it]                    \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[43mtrain_losses\u001b[49m\u001b[38;5;241m.\u001b[39mappend(epoch_loss)\n\u001b[1;32m    104\u001b[0m train_accuracies\u001b[38;5;241m.\u001b[39mappend(epoch_accuracy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define cross-validation iterator\n",
    "#skf = StratifiedShuffleSplit(n_splits=CV_FOLDS, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "# Determine the number of splits\n",
    "#n_splits = skf.get_n_splits(train_set.samples, train_set.targets)\n",
    "\n",
    "train_loss_crop = []\n",
    "train_loss_state = []\n",
    "train_accuracy_crop = []\n",
    "train_accuracy_state = []\n",
    "train_total = 0\n",
    "epoch_stats = []\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_crop_loss = 0.0\n",
    "    running_state_loss = 0.0\n",
    "    crop_correct = 0\n",
    "    state_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize tqdm for epoch progress\n",
    "    epoch_progress = tqdm(total=NUM_EPOCHS, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    '''\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_set.samples, train_set.targets)):\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    '''        \n",
    "    train_loader = torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE)\n",
    "    #val_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "        # Initialize tqdm for fold progress\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        img_paths = batch['img_path']\n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        images = []\n",
    "        for path, split in zip(img_paths, splits):\n",
    "            images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "        #fold_progress = tqdm(total=len(train_loader), desc=f'Fold {batch_idx + 1}/{len(train_loader)}', leave=False)\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "        #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "        inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "        crop_labels = crop_label_idx.clone().detach()\n",
    "        state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        crop_labels = crop_labels.to(device)\n",
    "        state_labels = state_labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "        crop_outputs, state_outputs = model(inputs)\n",
    "        \n",
    "        #crop_outputs = model_outputs[:, :len(full_set.unique_crops)]\n",
    "        #state_outputs = model_outputs[:, len(full_set.unique_states):]\n",
    "\n",
    "            # Calculate loss\n",
    "        crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        state_loss = criterion_state(state_outputs, state_labels)\n",
    "        \n",
    "        #running_loss = running_loss + crop_loss + state_loss    \n",
    "            # Backward pass and optimize\n",
    "        crop_loss.backward(retain_graph=True)\n",
    "        state_loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        _, predicted_crop = torch.max(crop_outputs, 1)\n",
    "        _, predicted_state = torch.max(state_outputs, 1)\n",
    "                    \n",
    "        crop_correct += (predicted_crop == crop_labels).sum().item()\n",
    "        state_correct += (predicted_state == state_labels).sum().item()\n",
    "        total += crop_labels.size(0)\n",
    "\n",
    "        running_crop_loss+= crop_loss.item()\n",
    "        running_state_loss += state_loss.item()\n",
    "\n",
    "            # Update tqdm progress description at batch level\n",
    "        #fold_progress.set_postfix(loss=running_loss / (batch_idx + 1), accuracy=100. * correct / total)\n",
    "        #fold_progress.set_postfix({})\n",
    "        #fold_progress.update(1)\n",
    "\n",
    "        # Close fold progress bar\n",
    "        #fold_progress.close()\n",
    "\n",
    "        # Update tqdm progress at fold level\n",
    "        epoch_progress.update(1)\n",
    "\n",
    "    # Close epoch progress bar\n",
    "    epoch_progress.close()\n",
    "\n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_crop_loss = running_crop_loss / len(train_loader)\n",
    "    epoch_state_loss = running_state_loss / len(train_loader)\n",
    "    epoch_crop_accuracy = 100. * crop_correct / total\n",
    "    epoch_state_accuracy = 100. * state_correct / total\n",
    "\n",
    "    # Log metrics\n",
    "    train_loss_crop.append(epoch_crop_loss)\n",
    "    train_loss_state.append(epoch_state_loss)\n",
    "    train_accuracy_crop.append(epoch_crop_accuracy)\n",
    "    train_accuracy_state.append(epoch_crop_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Crop:  0.9692628720210288\n",
      "Correct State:  0.4776407419865207\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct Crop: \", train_correct_crop/train_total)\n",
    "print(\"Correct State: \", train_correct_state/train_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/braines/.conda/envs/vgg/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "crop_correct = 0\n",
    "state_correct = 0\n",
    "model.eval()\n",
    "\n",
    "         #torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE)\n",
    "testing = torch.utils.data.DataLoader(full_set.test_samples)\n",
    "\n",
    "for batch_idx, batch in enumerate(testing):\n",
    "    crop_label_idx = batch['crop_idx']\n",
    "    img_paths = batch['img_path']\n",
    "    splits = batch['split']\n",
    "    state_label_idx = batch['state_idx']\n",
    "    images = []\n",
    "    for path, split in zip(img_paths, splits):\n",
    "        images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "    images_tensor = torch.stack(images, dim=0)\n",
    "    #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "    inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "    crop_labels = crop_label_idx.clone().detach()\n",
    "    state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    crop_labels = crop_labels.to(device)\n",
    "    state_labels = state_labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "    crop_outputs, state_outputs = model(inputs)\n",
    "\n",
    "    _, crop_predicted = crop_outputs.max(1)\n",
    "    _, state_predicted = state_outputs.max(1)\n",
    "    total += crop_labels.size(0)\n",
    "    crop_correct += crop_predicted.eq(crop_labels).sum().item()\n",
    "    state_correct += state_predicted.eq(state_labels).sum().item()\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    pred = model(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Test Accuracy:  3.058324326488131\n",
      "Crop Test Accuracy:  21.79656538969617\n"
     ]
    }
   ],
   "source": [
    "crop_accuracy = 100. * crop_correct / total\n",
    "state_accuracy = 100. * state_correct / total\n",
    "print(\"State Test Accuracy: \", state_accuracy)\n",
    "print(\"Crop Test Accuracy: \", crop_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for test_idx, (inputs, targets) in enumerate(testing):\n",
    "    print(targets)\n",
    "    count += 1\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: Union[str, Path]) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folders in a dataset.\n",
    "\n",
    "    See :class:`DatasetFolder` for details.\n",
    "    \"\"\"\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_classes(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_mean_and_sd(loader):\n",
    "    \"\"\"Compute the mean and sd in an online fashion\n",
    "\n",
    "        Var[x] = E[X^2] - E^2[X]\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for data in loader:\n",
    "\n",
    "        b, c, h, w = data.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(data, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(data ** 2, dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_set = torch.utils.data.DataLoader(full_set.test_sample, batch_size=1, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in normal_set:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(normal_set.dataset)\n",
    "std /= len(normal_set.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.4851, 0.5189, 0.3830])\n",
      "Std:  tensor([0.2000, 0.1880, 0.2216])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CropDoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
