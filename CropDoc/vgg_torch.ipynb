{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vgg_class\n",
    "from data import DatasetManager, TransformerManager\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, CPUOffload\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union\n",
    "from loguru import logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_standard_transforms = {\n",
    "    'train': transforms.Compose([transforms.Resize((224,224)),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([transforms.Resize((224,224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/train_Data/Cashew/\"\n",
    "#test_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/test_data/Cashew/\"\n",
    "root_dir = \"/scratch/braines/Dataset/CCMT-Dataset-Augmented/\"\n",
    "full_set = DatasetManager(root_dir, transform=data_standard_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5  # Number of passes through entire training dataset\n",
    "CV_FOLDS = 2  # Number of cross-validation folds\n",
    "BATCH_SIZE = 128  # Within each epoch data is split into batches\n",
    "LEARNING_RATE = 0.001\n",
    "VAL_SPLIT = 0.1\n",
    "CROSS_VALIDATE = True\n",
    "\n",
    "device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "vgg = vgg_class.vgg16((len(full_set.unique_crops), len(full_set.unique_states)))\n",
    "model = nn.DataParallel(vgg, device_ids=device_ids)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion_crop = nn.CrossEntropyLoss()\n",
    "criterion_state = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/5 [00:00<?, ?it/s]/home/braines/.conda/envs/vgg/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "Epoch 1/5: 63it [11:59, 11.42s/it]                       \n",
      "Epoch 2/5: 63it [11:46, 11.22s/it]                       \n",
      "Epoch 3/5: 63it [11:34, 11.02s/it]                       \n",
      "Epoch 4/5: 63it [11:11, 10.66s/it]                       \n",
      "Epoch 5/5: 63it [11:36, 11.06s/it]                       \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define cross-validation iterator\n",
    "#skf = StratifiedShuffleSplit(n_splits=CV_FOLDS, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "# Determine the number of splits\n",
    "#n_splits = skf.get_n_splits(full_set.samples, full_set.targets)\n",
    "\n",
    "train_loss_crop = []\n",
    "train_loss_state = []\n",
    "val_loss_crop = []\n",
    "val_loss_state = []\n",
    "train_accuracy_crop = []\n",
    "train_accuracy_state = []\n",
    "val_accuracy_crop = []\n",
    "val_accuracy_state = []\n",
    "train_total = 0\n",
    "epoch_stats = []\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_crop_loss = 0.0\n",
    "    running_state_loss = 0.0\n",
    "    val_running_crop_loss = 0.0\n",
    "    val_running_state_loss = 0.0\n",
    "    crop_correct = 0\n",
    "    val_crop_correct = 0\n",
    "    val_state_correct = 0\n",
    "    state_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize tqdm for epoch progress\n",
    "    epoch_progress = tqdm(total=NUM_EPOCHS, desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}')\n",
    "    '''\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(train_set.samples, train_set.targets)):\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "            \n",
    "    \n",
    "        val_loader = torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE, sampler=val_sampler, pin_memory=True)\n",
    "    '''\n",
    "    train, valid = random_split(full_set.train_samples, [1-VAL_SPLIT, VAL_SPLIT])\n",
    "    train_loader = DataLoader(train, batch_size=BATCH_SIZE ,pin_memory= True)\n",
    "    valid_loader = DataLoader(valid, batch_size=BATCH_SIZE ,pin_memory= True)\n",
    "            # Initialize tqdm for fold progress\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        img_paths = batch['img_path']\n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        images = []\n",
    "        for path, split in zip(img_paths, splits):\n",
    "            images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "            #fold_progress = tqdm(total=len(train_loader), desc=f'Fold {batch_idx + 1}/{len(train_loader)}', leave=False)\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "            #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "        inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "        crop_labels = crop_label_idx.clone().detach()\n",
    "        state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        crop_labels = crop_labels.to(device)\n",
    "        state_labels = state_labels.to(device)\n",
    "                # Zero the parameter gradients\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Forward pass\n",
    "        crop_outputs, state_outputs = model(inputs)\n",
    "            \n",
    "            #crop_outputs = model_outputs[:, :len(full_set.unique_crops)]\n",
    "            #state_outputs = model_outputs[:, len(full_set.unique_states):]\n",
    "\n",
    "                # Calculate loss\n",
    "        crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        state_loss = criterion_state(state_outputs, state_labels)\n",
    "            \n",
    "        total_loss = crop_loss + state_loss\n",
    "            #running_loss = running_loss + crop_loss + state_loss    \n",
    "                # Backward pass and optimize\n",
    "            #crop_loss.backward(retain_graph=True)\n",
    "            #state_loss.backward()\n",
    "            \n",
    "\n",
    "        _, predicted_crop = torch.max(crop_outputs, 1)\n",
    "        _, predicted_state = torch.max(state_outputs, 1)\n",
    "                        \n",
    "        crop_correct += (predicted_crop == crop_labels).sum().item()\n",
    "        state_correct += (predicted_state == state_labels).sum().item()\n",
    "        total += len(batch)\n",
    "\n",
    "        running_crop_loss+= crop_loss.item()\n",
    "        running_state_loss += state_loss.item()\n",
    "\n",
    "        total_loss.backward()\n",
    "            \n",
    "        optimiser.step()\n",
    "                # Update tqdm progress description at batch level\n",
    "            #fold_progress.set_postfix(loss=running_loss / (batch_idx + 1), accuracy=100. * correct / total)\n",
    "            #fold_progress.set_postfix({})\n",
    "            #fold_progress.update(1)\n",
    "\n",
    "            # Close fold progress bar\n",
    "            #fold_progress.close()\n",
    "\n",
    "            # Update tqdm progress at fold level\n",
    "            #if batch_idx == 1:\n",
    "            #    break\n",
    "    model.eval()\n",
    "    for batch_idx, batch in enumerate(valid_loader):\n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        img_paths = batch['img_path']\n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        images = []\n",
    "        for path, split in zip(img_paths, splits):\n",
    "            images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "        images_tensor = torch.stack(images, dim=0)\n",
    "            #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "        inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "        crop_labels = crop_label_idx.clone().detach()\n",
    "        state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        crop_labels = crop_labels.to(device)\n",
    "        state_labels = state_labels.to(device)\n",
    "\n",
    "        crop_outputs, state_outputs = model(inputs)\n",
    "\n",
    "        val_crop_loss = criterion_crop(crop_outputs, crop_labels)\n",
    "        val_state_loss = criterion_state(state_outputs, state_labels)\n",
    "\n",
    "        _, val_predicted_crop = torch.max(crop_outputs, 1)\n",
    "        _, val_predicted_state = torch.max(state_outputs, 1)\n",
    "\n",
    "        val_crop_correct += (val_predicted_crop == crop_labels).sum().item()\n",
    "        val_state_correct += (val_predicted_state == state_labels).sum().item()\n",
    "        total += len(batch)\n",
    "\n",
    "        val_running_crop_loss+= crop_loss.item()\n",
    "        val_running_state_loss += state_loss.item()\n",
    "\n",
    "        epoch_progress.update(1)\n",
    "\n",
    "    # Close epoch progress bar\n",
    "    epoch_progress.close()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Calculate epoch-level metrics\n",
    "    epoch_crop_loss = running_crop_loss / len(train_loader)\n",
    "    epoch_state_loss = running_state_loss / len(train_loader)\n",
    "    epoch_crop_accuracy = 100. * crop_correct / len(train)\n",
    "    epoch_state_accuracy = 100. * state_correct / len(train)\n",
    "    epoch_val_crop_loss = val_running_crop_loss / len(valid_loader)\n",
    "    epoch_val_state_loss = val_running_state_loss / len(valid_loader)\n",
    "    epoch_val_crop_accuracy = 100. * val_crop_correct / len(valid)\n",
    "    epoch_val_state_accuracy = 100. * val_state_correct / len(valid)\n",
    "\n",
    "    # Log metrics\n",
    "    train_loss_crop.append(epoch_crop_loss)\n",
    "    train_loss_state.append(epoch_state_loss)\n",
    "    train_accuracy_crop.append(epoch_crop_accuracy)\n",
    "    train_accuracy_state.append(epoch_state_accuracy)\n",
    "    val_loss_crop.append(epoch_val_crop_loss)\n",
    "    val_loss_state.append(epoch_val_state_loss)\n",
    "    val_accuracy_crop.append(epoch_val_crop_accuracy)\n",
    "    val_accuracy_state.append(epoch_val_state_accuracy)\n",
    "    print(\"Epoch: \", epoch + 1)\n",
    "    print(\"Train - Crop Loss: {}, State Loss: {}, Crop Accuracy: {}, State Accuracy: {}\".format(train_loss_crop, train_loss_state, train_accuracy_crop, train_accuracy_state))\n",
    "    print(\"Validate - Crop Loss: {}, State Loss: {}, Crop Accuracy: {}, State Accuracy: {}\".format(val_loss_crop, val_loss_state, val_accuracy_crop, val_accuracy_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop correct:  78774\n",
      "State correct:  27017\n",
      "Total:  80271\n",
      "crop_accuracy:  98.13506745898269\n"
     ]
    }
   ],
   "source": [
    "print(\"Crop correct: \", crop_correct)\n",
    "print(\"State correct: \", state_correct)\n",
    "print(\"Total: \", len(full_set.train_samples))\n",
    "print(\"crop_accuracy: \", 100. * crop_correct / len(full_set.train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y+6\n"
     ]
    }
   ],
   "source": [
    "y = 2\n",
    "print(\"y+6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "Crop Loss:  1.2459352704276025\n",
      "State Loss:  2.749915881283515\n",
      "Crop Accuracy:  50.53983721831571\n",
      "State Accuracy:  20.99136260450695\n",
      "Validation Crop Loss:  1.061446189880371\n",
      "Validation States Loss:  2.699982166290283\n",
      "Validation Crop Accuracy:  58.37797433661393\n",
      "Validation States Accuracy:  28.80279058178647\n",
      "Epoch:  2\n",
      "Crop Loss:  0.9305367541524161\n",
      "State Loss:  2.2799721538493065\n",
      "Crop Accuracy:  60.845191296163\n",
      "State Accuracy:  28.95465367366148\n",
      "Validation Crop Loss:  0.9835046529769897\n",
      "Validation States Loss:  2.45900559425354\n",
      "Validation Crop Accuracy:  64.74398903700012\n",
      "Validation States Accuracy:  28.877538308209793\n",
      "Epoch:  3\n",
      "Crop Loss:  0.8069555794243264\n",
      "State Loss:  2.0620343174554607\n",
      "Crop Accuracy:  67.39244781573557\n",
      "State Accuracy:  34.64509163390731\n",
      "Validation Crop Loss:  0.7115528583526611\n",
      "Validation States Loss:  1.7276532649993896\n",
      "Validation Crop Accuracy:  57.61803911797683\n",
      "Validation States Accuracy:  31.207175781736638\n",
      "Epoch:  4\n",
      "Crop Loss:  0.675626997610109\n",
      "State Loss:  1.8064118423293123\n",
      "Crop Accuracy:  74.15010243065169\n",
      "State Accuracy:  41.746027351752396\n",
      "Validation Crop Loss:  0.44347113370895386\n",
      "Validation States Loss:  1.4403612613677979\n",
      "Validation Crop Accuracy:  80.29151613305095\n",
      "Validation States Accuracy:  49.184003986545406\n",
      "Epoch:  5\n",
      "Crop Loss:  0.54235997600893\n",
      "State Loss:  1.5510228176032548\n",
      "Crop Accuracy:  79.89729250872045\n",
      "State Accuracy:  48.5784286584353\n",
      "Validation Crop Loss:  0.465952068567276\n",
      "Validation States Loss:  1.5935490131378174\n",
      "Validation Crop Accuracy:  70.42481624517254\n",
      "Validation States Accuracy:  42.36950292761929\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_EPOCHS):\n",
    "    print(\"Epoch: \", i+1)\n",
    "    print(\"Crop Loss: \", train_loss_crop[i])\n",
    "    print(\"State Loss: \", train_loss_state[i])\n",
    "    print(\"Crop Accuracy: \", train_accuracy_crop[i])\n",
    "    print(\"State Accuracy: \", train_accuracy_state[i])\n",
    "    print(\"Validation Crop Loss: \", val_loss_crop[i])\n",
    "    print(\"Validation States Loss: \", val_loss_state[i])\n",
    "    print(\"Validation Crop Accuracy: \", val_accuracy_crop[i])\n",
    "    print(\"Validation States Accuracy: \", val_accuracy_state[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/braines/.conda/envs/vgg/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "crop_correct = 0\n",
    "state_correct = 0\n",
    "model.eval()\n",
    "\n",
    "         #torch.utils.data.DataLoader(full_set.train_samples, batch_size=BATCH_SIZE)\n",
    "testing = torch.utils.data.DataLoader(full_set.test_samples)\n",
    "\n",
    "for batch_idx, batch in enumerate(testing):\n",
    "    crop_label_idx = batch['crop_idx']\n",
    "    img_paths = batch['img_path']\n",
    "    splits = batch['split']\n",
    "    state_label_idx = batch['state_idx']\n",
    "    images = []\n",
    "    for path, split in zip(img_paths, splits):\n",
    "        images.append(full_set.load_image_from_path(path, split))\n",
    "\n",
    "    images_tensor = torch.stack(images, dim=0)\n",
    "    #batch_metrics = train_batch(batch_idx, images_tensor, crop_label_idx, state_label_idx)\n",
    "    inputs = images_tensor.clone().detach().requires_grad_(True)\n",
    "    crop_labels = crop_label_idx.clone().detach()\n",
    "    state_labels = state_label_idx.clone().detach()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    crop_labels = crop_labels.to(device)\n",
    "    state_labels = state_labels.to(device)\n",
    "            # Zero the parameter gradients\n",
    "    optimiser.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "    crop_outputs, state_outputs = model(inputs)\n",
    "\n",
    "    _, crop_predicted = crop_outputs.max(1)\n",
    "    _, state_predicted = state_outputs.max(1)\n",
    "    total += crop_labels.size(0)\n",
    "    crop_correct += crop_predicted.eq(crop_labels).sum().item()\n",
    "    state_correct += state_predicted.eq(state_labels).sum().item()\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    pred = model(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Test Accuracy:  41.22733277290741\n",
      "Crop Test Accuracy:  71.81858212241303\n"
     ]
    }
   ],
   "source": [
    "crop_accuracy = 100. * crop_correct / len(full_set.test_samples)\n",
    "state_accuracy = 100. * state_correct / len(full_set.test_samples)\n",
    "print(\"State Test Accuracy: \", state_accuracy)\n",
    "print(\"Crop Test Accuracy: \", crop_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/braines/Dataset/CCMT-Dataset-Augmented/test_data/Cassava/bacterial blight/1991cassava_valid_bb.JPG']\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "['maize', 'cassava', 'cashew', 'tomato']\n",
      "tensor([10])\n",
      "['leaf miner', 'leaf curl', 'grasshoper', 'leaf spot', 'gmite', 'leaf blight', 'red rust', 'mosaic', 'verticulium wilt', 'streak virus', 'bb', 'farmyw', 'leaf beetle', 'gumosis', 'cashew healthy', 'cassava healthy', 'maize healthy', 'septoria leaf spot', 'tomato healthy', 'anthracnose', 'bspot']\n"
     ]
    }
   ],
   "source": [
    "crop_label_test = []\n",
    "state_label_test = []\n",
    "testing = torch.utils.data.DataLoader(full_set.test_samples)\n",
    "count = 0\n",
    "for batch_idx, batch in enumerate(testing):\n",
    "        \n",
    "        crop_label_idx = batch['crop_idx']\n",
    "        if crop_label_idx not in crop_label_test:\n",
    "                crop_label_test.append(crop_label_idx)\n",
    "        img_paths = batch['img_path']\n",
    "        \n",
    "        splits = batch['split']\n",
    "        state_label_idx = batch['state_idx']\n",
    "        if state_label_idx not in state_label_test:\n",
    "                state_label_test.append(state_label_idx)\n",
    "        \n",
    "        if batch_idx == 8000:\n",
    "                print(img_paths)\n",
    "                print(crop_label_idx)\n",
    "                print(crop_label_idx.clone().detach())\n",
    "                print(full_set.unique_crops)\n",
    "                print(state_label_idx)\n",
    "                print(full_set.unique_states)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crop_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "crop_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: Union[str, Path]) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folders in a dataset.\n",
    "\n",
    "    See :class:`DatasetFolder` for details.\n",
    "    \"\"\"\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = find_classes(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_mean_and_sd(loader):\n",
    "    \"\"\"Compute the mean and sd in an online fashion\n",
    "\n",
    "        Var[x] = E[X^2] - E^2[X]\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for data in loader:\n",
    "\n",
    "        b, c, h, w = data.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(data, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(data ** 2, dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    return fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_set = torch.utils.data.DataLoader(full_set.test_sample, batch_size=1, shuffle=False)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in normal_set:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "\n",
    "mean /= len(normal_set.dataset)\n",
    "std /= len(normal_set.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  tensor([0.4851, 0.5189, 0.3830])\n",
      "Std:  tensor([0.2000, 0.1880, 0.2216])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \", mean)\n",
    "print(\"Std: \", std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CropDoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
